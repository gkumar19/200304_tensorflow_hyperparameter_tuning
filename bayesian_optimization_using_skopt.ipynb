{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "# Data: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "# bayesian_optimization : https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "# http://krasserm.github.io/2018/03/21/bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Obseve dataset\n",
    "#1. size of csv\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.drop('Time', inplace=True, axis=1)\n",
    "y = df.pop('Class')\n",
    "X = df.copy()\n",
    "print('X:')\n",
    "print(X.head())\n",
    "print('y: ')\n",
    "print(y.head())\n",
    "print('X shape:')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. number of positive and negative sample\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))\n",
    "#1. mean, variance and scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=47)\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)\n",
    "#1. PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('explained variance w.r.t. number of features')\n",
    "#conclusion: no scope of reducing number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 define metrics\n",
    "#2. ROC, FP, FN, TP, TN, Confusion matrix, Accuracy\n",
    "from tensorflow.keras.metrics import TrueNegatives, TruePositives, AUC, FalseNegatives, FalsePositives\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "metrics = [TrueNegatives(), TruePositives(), AUC(), FalseNegatives(), FalsePositives(), Precision(), Recall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 create model\n",
    "#3. Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "def make_model(num_layers=2, num_nodes=10, activation='relu', batch_norm=False,\n",
    "               dropout=0.1, num_feature=29, num_target=1, learning_rate=0.001):\n",
    "    tf.keras.backend.clear_session()\n",
    "    layer_list = [Dense(num_nodes, activation=activation, input_shape=(num_feature,))]\n",
    "    if batch_norm == True:\n",
    "        layer_list.append(BatchNormalization())\n",
    "    layer_list.append(Dropout(dropout))\n",
    "    for _ in range(num_layers-2):\n",
    "        layer_list.append(Dense(num_nodes, activation=activation))\n",
    "        if batch_norm == True:\n",
    "            layer_list.append(BatchNormalization())\n",
    "        layer_list.append(Dropout(dropout))\n",
    "    layer_list.append(Dense(num_target, activation='sigmoid'))\n",
    "    model = Sequential(layer_list)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, metrics=metrics, optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "model = make_model(3, batch_norm=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 train model\n",
    "model.fit(X_train_scaled, y_train.values, epochs=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#4.1 optimum training time with optimum batch size\n",
    "import time\n",
    "print(time.time())\n",
    "def evaluate_training_time(batch_sizes):\n",
    "    recorded_time = []\n",
    "    for batch_size in batch_sizes:\n",
    "        t1 = time.time()\n",
    "        model.fit(X_train_scaled, y_train.values, epochs=5, batch_size=1000)\n",
    "        t2 = time.time()\n",
    "        recorded_time.append(t2-t1)\n",
    "    fig = px.line(x=batch_sizes, y=recorded_time).update_traces(mode='lines+markers')\n",
    "    fig.show()\n",
    "    return recorded_time\n",
    "\n",
    "batch_sizes = [np.power(2,i) for i in range(2,18)]\n",
    "evaluate_training_time(batch_sizes)\n",
    "#optimum time: 1024batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 tune model\n",
    "#5. hyperparameter tuning\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from skopt.utils import use_named_args\n",
    "tf.random.set_seed(47)\n",
    "\n",
    "import skopt\n",
    "from skopt.space.space import Categorical, Integer, Real\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_evaluations, plot_convergence, plot_objective\n",
    "\n",
    "dimensions = [Integer(low=3, high=7, name='num_layers'),\n",
    "              Integer(low=2, high=15, name='num_nodes'),\n",
    "              Categorical(categories=['relu', 'tanh'], name='activation'),\n",
    "              Categorical(categories=[True, False], name='batch_norm'),\n",
    "              Real(low=0, high=0.7, name='dropout'),\n",
    "              Real(low=0, high=100, name='class_weight'),\n",
    "              Real(low=0.001, high=1, name='learning_rate', prior='log-uniform')]\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(num_layers, num_nodes, activation, batch_norm, dropout, class_weight, learning_rate):\n",
    "    #class_weight = (0,5) --> (changed classes)\n",
    "    class_weight_0 = y_train.value_counts()[1]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "    class_weight_1 = y_train.value_counts()[0]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "    class_weight = {0: class_weight_0 + class_weight_0*class_weight,\n",
    "                    1: class_weight_1 - class_weight_0*class_weight}\n",
    "    \n",
    "\n",
    "    hparams = {'num_layers': num_layers.item(),\n",
    "               'num_nodes': num_nodes.item(),\n",
    "               'activation': activation,\n",
    "               'batch_norm': batch_norm,\n",
    "               'dropout': dropout,\n",
    "               'class_weight': class_weight[0],\n",
    "               'learning_rate': learning_rate\n",
    "                          } #for storing in the tensorboard\n",
    "    \n",
    "    log_seq = int(time.time())\n",
    "    logdir= r'logs\\t_{}'.format(log_seq)\n",
    "    model = make_model(num_layers=num_layers, num_nodes=num_nodes,\n",
    "                         activation=activation, batch_norm=batch_norm,\n",
    "                         dropout=dropout, learning_rate=learning_rate)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train.values, epochs=10, batch_size=1024,\n",
    "              callbacks=[tf.keras.callbacks.TensorBoard(logdir),\n",
    "                         hp.KerasCallback(logdir, hparams, trial_id=str(log_seq))],\n",
    "              class_weight=class_weight, verbose=0)\n",
    "    model.save(f'saved_models\\{log_seq}.h5')\n",
    "    fscore = precision_recall_fscore_support(y_test, model.predict(X_test_scaled)>0.5, average='binary')[2]\n",
    "    return -fscore #maximizing f score, since sklearn optimizer tries to minimize the fitness\n",
    "\n",
    "#tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = gp_minimize(func = fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls=50,\n",
    "                        n_random_starts=20,\n",
    "                        acq_func='EI',\n",
    "                        random_state=47,\n",
    "                        verbose=True,\n",
    "                        acq_optimizer='lbfgs',\n",
    "                        n_points=10000,\n",
    "                        n_restarts_optimizer=10,\n",
    "                        xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters: ', optimizer.x)\n",
    "print('best fitness: ', optimizer.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparameters in zip(optimizer.func_vals, optimizer.x_iters): #best hyperparameters\n",
    "    print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyperparameters in sorted(zip(optimizer.func_vals, optimizer.x_iters)): #best hyperparameters sorted\n",
    "    print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_objective(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_evaluations(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters:\n",
    "log_seq = int(time.time())\n",
    "logdir = r'logs_best_para'\n",
    "\n",
    "model = make_model(num_layers=4, num_nodes=5,\n",
    "                     activation='tanh', batch_norm=False,\n",
    "                     dropout=0.279, learning_rate=0.1940)\n",
    "\n",
    "class_weight = 35.44\n",
    "class_weight_0 = y_train.value_counts()[1]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "class_weight_1 = y_train.value_counts()[0]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "class_weight = {0: class_weight_0 + class_weight_0*class_weight,\n",
    "                1: class_weight_1 - class_weight_0*class_weight}\n",
    "\n",
    "model.fit(X_train_scaled, y_train.values, epochs=100, batch_size=1024,\n",
    "          callbacks=[tf.keras.callbacks.TensorBoard(logdir)],\n",
    "          class_weight=class_weight, validation_split=0.2)\n",
    "\n",
    "#tensorboard --logdir logs_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, model.predict(X_test_scaled)>0.5)\n",
    "sns.heatmap(cm, annot=True , fmt='d', cmap='Blues')\n",
    "plt.ylabel('actual')\n",
    "plt.xlabel('prediction')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 20)\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(labels, predictions):\n",
    "    fp, tp, thresholds = roc_curve(labels, predictions)\n",
    "    fig1 = px.line(x=100*fp, y=100*tp).update_traces(line_color='red')\n",
    "    fig2 = px.line(x=100*fp, y=100*thresholds).update_traces(line_color='yellow')\n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(fig1.data)\n",
    "    fig.add_traces(fig2.data)\n",
    "    fig.update_xaxes(title_text='False Positive')\n",
    "    fig.update_yaxes(title_text='True Positive')\n",
    "    fig.show()\n",
    "    print(fig1.data)\n",
    "plot_roc(y_test, model.predict(X_test_scaled))\n",
    "#conclusion: scope of improving False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, model.predict(X_test_scaled)>0.9)\n",
    "sns.heatmap(cm, annot=True , fmt='d', cmap='Blues')\n",
    "plt.ylabel('actual')\n",
    "plt.xlabel('prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
