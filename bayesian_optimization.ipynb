{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "# Data: https://www.kaggle.com/mlg-ulb/creditcardfraud      ",
    "# bayesian_optimization Package: https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Obseve dataset\n",
    "#1. size of csv\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.drop('Time', inplace=True, axis=1)\n",
    "y = df.pop('Class')\n",
    "X = df.copy()\n",
    "print('X:')\n",
    "print(X.head())\n",
    "print('y: ')\n",
    "print(y.head())\n",
    "print('X shape:')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. number of positive and negative sample\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))\n",
    "#1. mean, variance and scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=47)\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train)\n",
    "X_test_scaled = scale.transform(X_test)\n",
    "#1. PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('explained variance w.r.t. number of features')\n",
    "#conclusion: no scope of reducing number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 define metrics\n",
    "#2. ROC, FP, FN, TP, TN, Confusion matrix, Accuracy\n",
    "from tensorflow.keras.metrics import TrueNegatives, TruePositives, AUC, FalseNegatives, FalsePositives\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "metrics = [TrueNegatives(), TruePositives(), AUC(), FalseNegatives(), FalsePositives(), Precision(), Recall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 create model\n",
    "#3. Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "def make_nn_model(num_layers=2, num_nodes=10, activation='relu', batch_norm=False, dropout=0.1, num_feature=29, num_target=1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    layer_list = [Dense(num_nodes, activation=activation, input_shape=(num_feature,))]\n",
    "    if batch_norm == True:\n",
    "        layer_list.append(BatchNormalization())\n",
    "    layer_list.append(Dropout(dropout))\n",
    "    for _ in range(num_layers-2):\n",
    "        layer_list.append(Dense(num_nodes, activation=activation))\n",
    "        if batch_norm == True:\n",
    "            layer_list.append(BatchNormalization())\n",
    "        layer_list.append(Dropout(dropout))\n",
    "    layer_list.append(Dense(num_target, activation='sigmoid'))\n",
    "    model = Sequential(layer_list)\n",
    "    return model\n",
    "model = make_nn_model(3, batch_norm=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 train model\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(loss=loss, metrics=metrics, optimizer='adam')\n",
    "model.fit(X_train_scaled, y_train.values, epochs=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#4.1 optimum training time with optimum batch size\n",
    "import time\n",
    "print(time.time())\n",
    "def evaluate_training_time(batch_sizes):\n",
    "    recorded_time = []\n",
    "    for batch_size in batch_sizes:\n",
    "        t1 = time.time()\n",
    "        model.fit(X_train_scaled, y_train.values, epochs=5, batch_size=1000)\n",
    "        t2 = time.time()\n",
    "        recorded_time.append(t2-t1)\n",
    "    fig = px.line(x=batch_sizes, y=recorded_time).update_traces(mode='lines+markers')\n",
    "    fig.show()\n",
    "    return recorded_time\n",
    "\n",
    "batch_sizes = [np.power(2,i) for i in range(2,18)]\n",
    "evaluate_training_time(batch_sizes)\n",
    "#optimum time: 1024batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 tune model\n",
    "#5. hyperparameter tuning\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "tf.random.set_seed(47)\n",
    "\n",
    "def model_run(num_layers, num_nodes, activation, batch_norm, dropout, class_weight):\n",
    "    #num_layers = (2,6) --> (2,5)\n",
    "    num_layers = int(num_layers)\n",
    "    \n",
    "    #num_nodes = (5,21) --> (5, 20)\n",
    "    num_nodes = int(num_nodes)\n",
    "    \n",
    "    #activation = (0.01,1.99) --> ('relu', 'tanh')\n",
    "    activation = int(activation)\n",
    "    activation_dict = {0: 'relu', 1: 'tanh'}\n",
    "    activation = activation_dict[activation]\n",
    "    \n",
    "    #batch_norm = (0.01,1.99) --> ('True', 'False')\n",
    "    batch_norm = int(batch_norm)\n",
    "    batch_norm_dict = {0: True, 1: False}\n",
    "    batch_norm = batch_norm_dict[batch_norm]    \n",
    "    \n",
    "    #dropout = (0,0.1)\n",
    "    dropout = dropout\n",
    "    \n",
    "    #class_weight = (0,5) --> (changed classes)\n",
    "    class_weight_0 = y_train.value_counts()[1]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "    class_weight_1 = y_train.value_counts()[0]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "    class_weight = {0: class_weight_0 + class_weight_0*class_weight,\n",
    "                    1: class_weight_1 - class_weight_0*class_weight}\n",
    "    \n",
    "    \n",
    "    \n",
    "    hparams = {'num_layers': num_layers,\n",
    "               'num_nodes': num_nodes,\n",
    "               'activation': activation,\n",
    "               'batch_norm': batch_norm,\n",
    "               'dropout': dropout,\n",
    "               'class_weight': class_weight[0]\n",
    "                          }\n",
    "    log_seq = int(time.time())\n",
    "    logdir= r'logs\\t_{}'.format(log_seq)\n",
    "    model = make_nn_model(num_layers=num_layers, num_nodes=num_nodes,\n",
    "                         activation=activation, batch_norm=batch_norm,\n",
    "                         dropout=dropout)\n",
    "    \n",
    "    model.compile(loss=loss, metrics=metrics, optimizer='adam')\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train.values, epochs=10, batch_size=1024,\n",
    "              callbacks=[tf.keras.callbacks.TensorBoard(logdir),\n",
    "                         hp.KerasCallback(logdir, hparams, trial_id=str(log_seq))],\n",
    "              class_weight=class_weight, verbose=0)\n",
    "    fscore = precision_recall_fscore_support(y_test, model.predict(X_test_scaled)>0.5, average='binary')[2]\n",
    "    return fscore #maximizing f score\n",
    "\n",
    "\n",
    "#tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "# Supress NaN warnings\n",
    "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'num_layers': (2,6),\n",
    "           'num_nodes': (5,21),\n",
    "           'activation': (0.01,1.99),\n",
    "           'batch_norm': (0.01,1.99),\n",
    "           'dropout': (0,0.5),\n",
    "           'class_weight': (0,50)\n",
    "                      }\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=model_run,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum \n",
    "    # is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=15)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)\n",
    "#optimizer.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters:\n",
    "log_seq = int(time.time())\n",
    "logdir = r'logs_best_para'\n",
    "model = make_nn_model(num_layers=4, num_nodes=10,\n",
    "                     activation='relu', batch_norm=True,\n",
    "                     dropout=0.2666)\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer='adam')\n",
    "\n",
    "class_weight = 48\n",
    "class_weight_0 = y_train.value_counts()[1]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "class_weight_1 = y_train.value_counts()[0]/(y_train.value_counts()[0]+ y_train.value_counts()[1])\n",
    "class_weight = {0: class_weight_0 + class_weight_0*class_weight,\n",
    "                1: class_weight_1 - class_weight_0*class_weight}\n",
    "\n",
    "model.fit(X_train_scaled, y_train.values, epochs=100, batch_size=1024,\n",
    "          callbacks=[tf.keras.callbacks.TensorBoard(logdir),\n",
    "                     hp.KerasCallback(logdir, hparams, trial_id=str(log_seq))],\n",
    "          class_weight=class_weight, validation_split=0.2)\n",
    "\n",
    "#tensorboard --logdir logs_best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, model.predict(X_test_scaled)>0.5)\n",
    "sns.heatmap(cm, annot=True , fmt='d', cmap='Blues')\n",
    "plt.ylabel('actual')\n",
    "plt.xlabel('prediction')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 20)\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(labels, predictions):\n",
    "    fp, tp, thresholds = roc_curve(labels, predictions)\n",
    "    fig1 = px.line(x=100*fp, y=100*tp).update_traces(line_color='red')\n",
    "    fig2 = px.line(x=100*fp, y=100*thresholds).update_traces(line_color='yellow')\n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(fig1.data)\n",
    "    fig.add_traces(fig2.data)\n",
    "    fig.update_xaxes(title_text='False Positive')\n",
    "    fig.update_yaxes(title_text='True Positive')\n",
    "    fig.show()\n",
    "    print(fig1.data)\n",
    "plot_roc(y_test, model.predict(X_test_scaled))\n",
    "#conclusion: scope of improving False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, model.predict(X_test_scaled)>0.95)\n",
    "sns.heatmap(cm, annot=True , fmt='d', cmap='Blues')\n",
    "plt.ylabel('actual')\n",
    "plt.xlabel('prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
